# -*- coding: utf-8 -*-
"""Data cleaning with pandas.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14I0A_H2bqZQSQqMfwof4bIvo8qUc7EyL
"""

'''
In this project, I will use the results from my LinkedIn job postings web scraper project which you can find in this 
repository https://github.com/ayanwu98/LinkedIn-Jobs-Web-Scraper-Project and perform data cleaning with Pandas and 
after the data has been cleaned up, I will perform some data explotarion to extract interesting information regarding the 
Data Science job market.

'''

# We first import the necessary libraries which are pandas and sqlite3 

import pandas as pd
import sqlite3 as sql

# We create a Pandas Dataframe from the results obtained in the web scraping project

df = pd.read_csv('/content/linkedinwebscraper.csv')

# We check the general information about our data frame. We see that there are no null values, so 
# we will not need to fill or drop rows with null values

df.info()

# Now, we will first explore job locations
# We see that the majority of jobs are located in the cities of Toronto, Vancouver, and Montreal.
df['Location'].value_counts()

# However, notice that there are 47 jobs in "Canada". Since Toronto is the city with the most jobs
# We will set the jobs in "Canada" to be in "Toronto".
for job in range(len(df)):
  if df['Location'][job] == 'Canada':
    df['Location'][job] = 'Toronto, Ontario, Canada'

# Now, while there are 900 jobs, some of them do not appear to be Data Science related. For instance, some of them are software engineering.
# We will then filter out this jobs.

for job in range(len(df)):
  if (('computer vision'.casefold() not in df['Job_Title'][job].casefold()) and ('nlp'.casefold() not in df['Job_Title'][job].casefold()) and ('data'.casefold() not in df['Job_Title'][job].casefold()) and ('Machine learning'.casefold() not in df['Job_Title'][job].casefold())):
    df = df.drop(index=job)
df.reset_index(drop=True)

# We proceed to drop duplicates

df = df.drop_duplicates()
df = df.reset_index(drop=True)

# We can see now that we end up with 204 unique jobs that are in the field of Data Science.
df.info()

# Now we create an csv file with the clean data
df.to_csv('linkedin_jobs_cleaned.csv', index=False)

# We will now proceed to perform data explotarion with SQL. 
# We will perform SQL queries in Python using the sqlite3 library.

# We first create a database named 'data_jobs'
conn = sql.connect('data_jobs.db')
c = conn.cursor()

# Now we create a table named jobs with the same columns.
c.execute("CREATE TABLE jobs(Company,Job_Title, Location, Seniority_level, Python, SQL, R , Excel, Tableau, Power_BI, Machine_Learning)")
jobs = pd.read_csv('/content/linkedin_jobs_cleaned.csv')
jobs.to_sql('jobs', conn, if_exists='append', index= False)

# As a first interesting query, we will see how many jobs there are that only require Excel and one of the BI tools.

# Now we select the jobs that are entry level and require excel and tableau or power BI skills.
# Not surprisingly, there is only one job. 

c.execute('''SELECT company, job_title
             FROM jobs
             WHERE (Seniority_level like '%entry%' AND (Excel AND tableau = TRUE) AND (Python AND R AND POWER_BI AND MACHINE_LEARNING = FALSE))
             OR (Seniority_level like '%entry%' AND (Excel AND power_BI = TRUE) AND (Python AND R AND tableau AND MACHINE_LEARNING = FALSE))
''')
c.fetchall()

# Now, we will extract the amount of jobs that require each skill
# We use a for loop to extract the amount of jobs requiring each individual skill
# We first create a list with the names of the skills

skills = ['Python', 'SQL', 'R' , 'Excel', 'Tableau', 'Power_BI', 'Machine_Learning']
number_jobs = []
for skill in range(len(skills)):
  c.execute('''SELECT count(company)
                FROM jobs
                WHERE {} = TRUE
  '''.format(skills[skill]))
  number_job = str(c.fetchall()[0]).replace('(','').replace(')','').replace(',','')
  number_jobs.append(number_job)
  print(f'''
  Skill: {skills[skill]}    
  Number of jobs: {number_job}
  ''')

for i in range(len(number_jobs)):
  int(number_jobs[i])

skills_df = pd.DataFrame({'skills': skills})
skills_df['number_of_jobs'] = number_jobs

skills_df.to_csv('number_of_jobs_per_skill.csv')

# We now write a query to see the number of jobs located in the cities of Toronto, Montreal, Vancouver, and Ottawa 
# since these are the cities with the most amount of jobs.

cities = ['Toronto', 'Montreal','Vancouver', 'Ottawa']
jobs_in_cities = []
for city in range(len(cities)):
  c.execute('''SELECT count(company)
                FROM jobs
                WHERE location like '%{}%'
  '''.format(cities[city]))
  jobs_in_city = str(c.fetchall()[0]).replace('(','').replace(')','').replace(',','')
  jobs_in_cities.append(jobs_in_city)
  print(f'''
  City: {cities[city]}    
  Number of jobs: {jobs_in_city}
  ''')

for i in range(len(jobs_in_cities)):
  int(jobs_in_cities[i])

cities_df = pd.DataFrame({'cities': cities})
cities_df['number_of_jobs'] = jobs_in_cities
cities_df.to_csv('number_of_jobs_per_city.csv')

""" We now write a query to see the number of jobs located in the cities of Toronto, Montreal, and Vancouver.
    But before we can do so, we need to extract a list of the different seniority levels.

    We first perform a SQL query to extract a list of the different seniority levels.
    Then, we wil extract a list containing only the unique seniority levels.

    Finally, we will run a SQL query to compute the number of jobs for each seniority level.
"""

# Query to extract a list of seniority levels
c.execute('''SELECT seniority_level
                FROM jobs
''')
seniority_levels = c.fetchall()

# We get a list of unique seniority levels 
seniority_levels_set = set(seniority_levels)
unique_seniority_levels = (list(seniority_levels_set))

# For loop to clean our list 
for level in range(len(unique_seniority_levels)):
  unique_seniority_levels[level] = str(unique_seniority_levels[level]).replace('(','').replace(')','').replace(',','').replace("'",'')

# SQL query to compute number of jobs per level
seniority_level_jobs = []
for level in range(len(unique_seniority_levels)):
  c.execute('''SELECT count(company)
                FROM jobs
                WHERE seniority_level like '%{}%'
  '''.format(unique_seniority_levels[level]))
  seniority_level_job = str(c.fetchall()[0]).replace('(','').replace(')','').replace(',','')
  seniority_level_jobs.append(seniority_level_job)
  print(f'''
  Seniority Level: {unique_seniority_levels[level]}    
  Number of jobs: {seniority_level_job}
  ''')

for i in range(len(seniority_level_jobs)):
  int(seniority_level_jobs[i])

seniority_level_df = pd.DataFrame({'seniority levels': unique_seniority_levels})
seniority_level_df['number_of_jobs'] = seniority_level_jobs
seniority_level_df.to_csv('number_of_jobs_per_level.csv')

